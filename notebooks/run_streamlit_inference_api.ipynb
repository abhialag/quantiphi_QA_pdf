{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "202d0bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install streamlit streamlit-extras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7bdbab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import subprocess\n",
    "import streamlit as st\n",
    "import logging\n",
    "from langchain.vectorstores import Chroma\n",
    "from constants import CHROMA_SETTINGS, EMBEDDING_MODEL_NAME, PERSIST_DIRECTORY, MODEL_ID, MODEL_BASENAME\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings,HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
    "# from streamlit_extras.add_vertical_space import add_vertical_space\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.schema.document import Document\n",
    "from langchain.storage import InMemoryStore\n",
    "\n",
    "from run_inference import load_model,retrieval_qa_pipeline\n",
    "from prompt_template_utils import get_prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "883b1748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up params:\n",
    "device_type='cpu'\n",
    "show_sources=True\n",
    "use_history=True\n",
    "save_qa=True\n",
    "promptTemplate_type=\"llama\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a84df9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Display Source Documents set to: True\n",
      "Display Use History set to: True\n"
     ]
    }
   ],
   "source": [
    "logging.info(f\"Display Source Documents set to: {show_sources}\")\n",
    "print(f\"Display Source Documents set to: {show_sources}\")\n",
    "logging.info(f\"Display Use History set to: {use_history}\")\n",
    "print(f\"Display Use History set to: {use_history}\")\n",
    "logging.info(f\"Display promptTemplate_type set to: {promptTemplate_type}\")\n",
    "logging.info(f\"Display Save QA set to: {save_qa}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c57539c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "QA,EMBEDDINGS,RETRIEVER,DB,LLM = retrieval_qa_pipeline(use_history, promptTemplate_type=promptTemplate_type,device_type=device_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df27e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading each objects into streamlit session states:\n",
    "\n",
    "if \"EMBEDDINGS\" not in st.session_state:\n",
    "    st.session_state.EMBEDDINGS = EMBEDDINGS\n",
    "\n",
    "if \"DB\" not in st.session_state:\n",
    "    st.session_state.DB = DB\n",
    "\n",
    "if \"RETRIEVER\" not in st.session_state:\n",
    "    st.session_state.RETRIEVER = RETRIEVER\n",
    "\n",
    "if \"LLM\" not in st.session_state:\n",
    "    st.session_state[\"LLM\"] = LLM\n",
    "\n",
    "\n",
    "if \"QA\" not in st.session_state:\n",
    "    st.session_state[\"QA\"] = QA\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73268335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sidebar contents\n",
    "\n",
    "with st.sidebar:\n",
    "    st.title(\"ðŸ¤—ðŸ’¬ I am a GenAI bot trained on Biology Concepts - plz ask me anything related?\")\n",
    "    st.markdown(\n",
    "        \"\"\"\n",
    "    ## About\n",
    "    Developed by Abhay Kumar for Quantiphi Interview Round.\n",
    "    This app is an LLM-powered chatbot capable of answering questions on Biology Textbook -Chapter 1-2.\n",
    " \n",
    "    \"\"\"\n",
    "    )\n",
    "    #add_vertical_space(5)\n",
    "    #st.write(\"Made by Abhay Kumar\")\n",
    "    \n",
    "    \n",
    "st.title(\"Biology_QA_bot ðŸ’¬\")\n",
    "# Create a text input box for the user\n",
    "prompt = st.text_input(\"Input your prompt here\")\n",
    "# while True:\n",
    "\n",
    "# If the user hits enter\n",
    "if prompt:\n",
    "    # Then pass the prompt to the LLM\n",
    "    response = st.session_state[\"QA\"](prompt)\n",
    "    answer, docs = response[\"result\"], response[\"source_documents\"]\n",
    "    # ...and write it out to the screen\n",
    "    st.write(answer)\n",
    "\n",
    "    # With a streamlit expander\n",
    "    with st.expander(\"Document Similarity Search\"):\n",
    "        # Find the relevant pages\n",
    "        search = st.session_state.DB.similarity_search_with_score(prompt)\n",
    "        # Write out the first\n",
    "        for i, doc in enumerate(search):\n",
    "            # print(doc)\n",
    "            st.write(f\"Source Document # {i+1} : {doc[0].metadata['source'].split('/')[-1]}\")\n",
    "            st.write(doc[0].page_content)\n",
    "            st.write(\"--------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
